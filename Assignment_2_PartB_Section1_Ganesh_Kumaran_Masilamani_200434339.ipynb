{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Assignment_2_PartB_Section1_Ganesh_Kumaran_Masilamani_200434339.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9de9e172be5042d5852f28f775ba00cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f2351bee965c479a8fc4748faa25b7e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a03a690f20f240d5971fa6c2a52129ec",
              "IPY_MODEL_7afc4f90938c4ad988c1b7588145eda8"
            ]
          }
        },
        "f2351bee965c479a8fc4748faa25b7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a03a690f20f240d5971fa6c2a52129ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5dd5cff3b0df4efea4f8b579f3df3b49",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_964bd6ccca9a4736bcfee0033cbd2c02"
          }
        },
        "7afc4f90938c4ad988c1b7588145eda8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7a00e013c384ed784ac40bba90a437c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 223kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad0e17cfc86b4b3d99eedb4af2620201"
          }
        },
        "5dd5cff3b0df4efea4f8b579f3df3b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "964bd6ccca9a4736bcfee0033cbd2c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7a00e013c384ed784ac40bba90a437c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad0e17cfc86b4b3d99eedb4af2620201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ee6625b093149ab9f3cbbe38c1230d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0bbdffb6ce004f93ba410245055e013e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6f82adf41cca4ad49725c59798755e06",
              "IPY_MODEL_8b8f92c609404f91ad61123072c2ac2f"
            ]
          }
        },
        "0bbdffb6ce004f93ba410245055e013e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f82adf41cca4ad49725c59798755e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba7ad5ea93bf4bbcaa62811c4da32863",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f89ee0638af24253b282584f978c2554"
          }
        },
        "8b8f92c609404f91ad61123072c2ac2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33814e1c82cf4e9eba7a50b5b71ce941",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 58.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f63d88ae5f294ca09c8514979daed9d2"
          }
        },
        "ba7ad5ea93bf4bbcaa62811c4da32863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f89ee0638af24253b282584f978c2554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33814e1c82cf4e9eba7a50b5b71ce941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f63d88ae5f294ca09c8514979daed9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebc13e7bc18e40059c526e93f6b02225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c4652f5766bb440f84322980816013eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1b1117447f0c40c6b8291cda4758d54b",
              "IPY_MODEL_532c73cde7384761927889daa8de148f"
            ]
          }
        },
        "c4652f5766bb440f84322980816013eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b1117447f0c40c6b8291cda4758d54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec9e9cebddbe43a1a364056280a4865b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46870c72df2b4391acafec04110dd6ef"
          }
        },
        "532c73cde7384761927889daa8de148f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c3faec9f068442abddb64116cfb54ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 2.47MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f26d631e3ce745b1b7d769df85acb095"
          }
        },
        "ec9e9cebddbe43a1a364056280a4865b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46870c72df2b4391acafec04110dd6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c3faec9f068442abddb64116cfb54ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f26d631e3ce745b1b7d769df85acb095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f6ac1409a6f435f9b4a24de9543d07c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4ddb99db850e4c17a97f567e114ce094",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e64b21e847a44699860c4acbc9a2d990",
              "IPY_MODEL_60db75f05d144115bb61f0abd66423d8"
            ]
          }
        },
        "4ddb99db850e4c17a97f567e114ce094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e64b21e847a44699860c4acbc9a2d990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a8102393adba4f4a87da4ad365d05068",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86b104c09c4646358f510c2b8e017d4f"
          }
        },
        "60db75f05d144115bb61f0abd66423d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_352b71afb13c425e9a2cde4f81bdbaa2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:08&lt;00:00, 41.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e7c0c21d5e547d4b40bfbd2f22098b5"
          }
        },
        "a8102393adba4f4a87da4ad365d05068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86b104c09c4646358f510c2b8e017d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "352b71afb13c425e9a2cde4f81bdbaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e7c0c21d5e547d4b40bfbd2f22098b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWgujXmGqzIC"
      },
      "source": [
        "##**Asssignment2-PART-B-Section1-Sentence Classification with BERT**\n",
        "\n",
        "**NAME :** Ganesh Kumaran Masilamani\n",
        "\n",
        "**STUDENT ID :** 200434339"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4spJ5PRhGJ1l"
      },
      "source": [
        "#libraries imported\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AElpzsKFiZSo",
        "outputId": "8a01a5f4-a26a-4419-de6d-e4924b5a1689"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=0fd21c230387a44351d458632c7dbabf9c822dd8f3325148e399b1907e048f4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbamBD0xu5z"
      },
      "source": [
        "##**Preprocessing and Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "9de9e172be5042d5852f28f775ba00cd",
            "f2351bee965c479a8fc4748faa25b7e8",
            "a03a690f20f240d5971fa6c2a52129ec",
            "7afc4f90938c4ad988c1b7588145eda8",
            "5dd5cff3b0df4efea4f8b579f3df3b49",
            "964bd6ccca9a4736bcfee0033cbd2c02",
            "f7a00e013c384ed784ac40bba90a437c",
            "ad0e17cfc86b4b3d99eedb4af2620201",
            "2ee6625b093149ab9f3cbbe38c1230d7",
            "0bbdffb6ce004f93ba410245055e013e",
            "6f82adf41cca4ad49725c59798755e06",
            "8b8f92c609404f91ad61123072c2ac2f",
            "ba7ad5ea93bf4bbcaa62811c4da32863",
            "f89ee0638af24253b282584f978c2554",
            "33814e1c82cf4e9eba7a50b5b71ce941",
            "f63d88ae5f294ca09c8514979daed9d2",
            "ebc13e7bc18e40059c526e93f6b02225",
            "c4652f5766bb440f84322980816013eb",
            "1b1117447f0c40c6b8291cda4758d54b",
            "532c73cde7384761927889daa8de148f",
            "ec9e9cebddbe43a1a364056280a4865b",
            "46870c72df2b4391acafec04110dd6ef",
            "3c3faec9f068442abddb64116cfb54ce",
            "f26d631e3ce745b1b7d769df85acb095"
          ]
        },
        "id": "hKj6Y_TydjeS",
        "outputId": "a3653cb5-7851-47d4-b246-3dee6a585804"
      },
      "source": [
        "from transformers import DistilBertTokenizer, RobertaTokenizer \n",
        "import tqdm\n",
        "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
        "\n",
        "# Defining DistilBERT tokonizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
        "                                                max_length=128, pad_to_max_length=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, pad_length=128, pad_to_max_length=True ):\n",
        "    if type(sentences) == str:\n",
        "        inputs = tokenizer.encode_plus(sentences, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        return np.asarray(inputs['input_ids'], dtype='int32'), np.asarray(inputs['attention_mask'], dtype='int32'), np.asarray(inputs['token_type_ids'], dtype='int32')\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9de9e172be5042d5852f28f775ba00cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ee6625b093149ab9f3cbbe38c1230d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebc13e7bc18e40059c526e93f6b02225",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRoKe2DKyi41",
        "outputId": "c472c27d-2680-4fb9-97d1-9e4b0317f58e"
      },
      "source": [
        "inputs = tokenizer.tokenize(\"The capital of France is [MASK].\")\n",
        "print(inputs,'\\n')\n",
        "\n",
        "inputs = tokenizer.tokenize(\"This is a pretrained model.\")\n",
        "print(inputs,'\\n')\n",
        "\n",
        "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer)\n",
        "print(ids)\n",
        "print(masks,\"\\n\")\n",
        "\n",
        "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer, pad_to_max_length=False)\n",
        "print(ids)\n",
        "print(masks)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['the', 'capital', 'of', 'france', 'is', '[MASK]', '.'] \n",
            "\n",
            "['this', 'is', 'a', 'pre', '##train', '##ed', 'model', '.'] \n",
            "\n",
            "[ 101 1996 3007 1997 2605 2003  103 1012  102    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
            "\n",
            "[ 101 1996 3007 1997 2605 2003  103 1012  102]\n",
            "[1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6OuZAA8sbdg"
      },
      "source": [
        "##**Loading Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "###**Downloading and preprocessing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyuSzkafqNca",
        "outputId": "8e17c42f-b64d-4137-a0fb-6dd2d4ab0629"
      },
      "source": [
        "import requests\n",
        "def downloadfile(url):\n",
        "  rq = requests.get(url)\n",
        "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
        "\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016devtest-BD.tsv')\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016dev-BD.tsv')\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016test-BD.tsv')\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016train-BD.tsv')\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2015test-BD.tsv')\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2015train-BD.tsv')\n",
        "\n",
        "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/gold/SemEval2017-task4-test.subtask-BD.english.txt')\n",
        "\n",
        "with open('twitter-2016dev-BD.tsv', 'r') as f:\n",
        "  dev_original = [l.strip().split('\\t') for l in f.readlines()]\n",
        "with open('SemEval2017-task4-test.subtask-BD.english.txt', 'r') as f:\n",
        "  test_original = [l.strip().split('\\t') for l in f.readlines()]\n",
        "train_original = []\n",
        "with open('twitter-2016train-BD.tsv', 'r') as f:\n",
        "  train_original = [l.strip().split('\\t') for l in f.readlines()]\n",
        "with open('twitter-2016test-BD.tsv', 'r') as f:\n",
        "  train_original.extend([l.strip().split('\\t') for l in f.readlines()])\n",
        "with open('twitter-2016devtest-BD.tsv', 'r') as f:\n",
        "  train_original.extend([l.strip().split('\\t') for l in f.readlines()])\n",
        "with open('twitter-2015test-BD.tsv', 'r') as f:\n",
        "  train_original.extend([l.strip().split('\\t') for l in f.readlines() if l.strip().split('\\t')[2] in ['negative','positive']])\n",
        "with open('twitter-2015train-BD.tsv', 'r') as f:\n",
        "  train_original.extend([l.strip().split('\\t') for l in f.readlines() if l.strip().split('\\t')[2] in ['negative','positive']])\n",
        "\n",
        "print(\"Training entries: {}\".format(len(train_original)))\n",
        "print(\"Development entries: {}\".format(len(dev_original)))\n",
        "print(\"Testing entries: {}\".format(len(test_original)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 17639\n",
            "Development entries: 1325\n",
            "Testing entries: 6185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gjWRAuqg5s",
        "outputId": "d36ee295-8c7c-4ad8-9251-251f196caedf"
      },
      "source": [
        "print(\"ID \\t TOPIC \\t LABLE \\t TWEET_TEXT\")\n",
        "print(train_original[0])\n",
        "print(train_original[1])\n",
        "print(train_original[2])\n",
        "print(train_original[3])\n",
        "print(train_original[4])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID \t TOPIC \t LABLE \t TWEET_TEXT\n",
            "['628949369883000832', '@microsoft', 'negative', \"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\"]\n",
            "['628976607420645377', '@microsoft', 'negative', \"@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\"]\n",
            "['629023169169518592', '@microsoft', 'negative', \"I may be ignorant on this issue but... should we celebrate @Microsoft's parental leave changes? Doesn't the gender divide suggest... (1/2)\"]\n",
            "['629179223232479232', '@microsoft', 'negative', 'Thanks to @microsoft, I just may be switching over to @apple.']\n",
            "['629226490152914944', '@microsoft', 'positive', 'Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCH1OoDrSNR",
        "outputId": "fa2aa532-71de-4a8c-f75c-5feab5821764"
      },
      "source": [
        "# Please write your code to generate the following data\n",
        "# your code goes here\n",
        "#FOR TRAIN DATA\n",
        "x_train_tweet_int= []\n",
        "x_train_tweet_masks = []\n",
        "x_train_topic_int = []\n",
        "x_train_topic_masks = []\n",
        "for train_data in range(len(train_original)):\n",
        "  train_ids,train_masks,train_segments = tokenize(train_original[train_data][-1],tokenizer)\n",
        "  #Append the train tweet_int and tweet_masks to ids and masks\n",
        "  x_train_tweet_int.append(train_ids)\n",
        "  x_train_tweet_masks.append(train_masks)\n",
        "  train_ids,train_masks,train_segments = tokenize(train_original[train_data][1],tokenizer)\n",
        "  #Append the train topic_int and topic_masks to ids and masks\n",
        "  x_train_topic_int.append(train_ids)\n",
        "  x_train_topic_masks.append(train_masks)\n",
        "\n",
        "#FOR DEV DATA\n",
        "x_dev_tweet_int= []\n",
        "x_dev_tweet_masks = []\n",
        "x_dev_topic_int = []\n",
        "x_dev_topic_masks = []\n",
        "for dev_data in range(len(dev_original)):\n",
        "  dev_ids,dev_masks,dev_segments = tokenize(dev_original[dev_data][-1],tokenizer)\n",
        "  #Append the dev tweet_int and tweet_masks to ids and masks\n",
        "  x_dev_tweet_int.append(dev_ids)\n",
        "  x_dev_tweet_masks.append(dev_masks)\n",
        "  dev_ids,dev_masks,dev_segments = tokenize(dev_original[dev_data][1], tokenizer)\n",
        "  #Append the dev topic_int and topic_masks to ids and masks\n",
        "  x_dev_topic_int.append(dev_ids)\n",
        "  x_dev_topic_masks.append(dev_masks)\n",
        "\n",
        "#FOR TEST DATA\n",
        "x_test_tweet_int= []\n",
        "x_test_tweet_masks = []\n",
        "x_test_topic_int = []\n",
        "x_test_topic_masks = []\n",
        "for test_data in range(len(test_original)):\n",
        "  test_ids,test_masks,test_segments = tokenize(test_original[test_data][-1],tokenizer)\n",
        "  #Append the test tweet_int and tweet_masks to ids and masks\n",
        "  x_test_tweet_int.append(test_ids)\n",
        "  x_test_tweet_masks.append(test_masks)\n",
        "  test_ids,test_masks,test_segments = tokenize(test_original[test_data][1],tokenizer )\n",
        "  #Append the test topic_int and topic_masks to ids and masks\n",
        "  x_test_topic_int.append(test_ids)\n",
        "  x_test_topic_masks.append(test_masks)\n",
        "\n",
        "\n",
        "# If use the previous tokenize function, you can get a print result like:\n",
        "assert len(x_train_topic_int) == len(train_original)\n",
        "assert len(x_train_topic_masks) == len(x_train_topic_int)\n",
        "assert len(x_test_topic_int) == len(test_original)\n",
        "assert len(x_test_topic_masks) == len(x_test_topic_int)\n",
        "print(\"x_dev_topic_int[0]:\")\n",
        "print(x_dev_topic_int[0])\n",
        "print(\"x_dev_topic_masks[0]:\")\n",
        "print(x_dev_topic_masks[0])\n",
        "print(\"x_dev_tweet_int[0]:\")\n",
        "print(x_dev_tweet_int[0])\n",
        "print(\"x_dev_tweet_masks[0]:\")\n",
        "print(x_dev_tweet_masks[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_dev_topic_int[0]:\n",
            "[ 101 2745 4027  102    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "x_dev_topic_masks[0]:\n",
            "[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "x_dev_tweet_int[0]:\n",
            "[  101  6108  1062  9794 16021 23091  2007 16839  9080 12863  7050  2000\n",
            "  2745  4027  1024  6108  1062  4593  2587 16021 23091  2006  5095  1998\n",
            "  1012  1012  8299  1024  1013  1013  1056  1012  2522  1013  1053  3501\n",
            "  2683  2072  2549  8586  2615 18037   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "x_dev_tweet_masks[0]:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IreFXgruZot"
      },
      "source": [
        "We use 1 to represent \"positive\" and 0 for \"negative\" and generate the \"y\" data similar to previous labs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abIb7Fe5u3GQ",
        "outputId": "f6837d9e-0310-4deb-f885-012f84784f36"
      },
      "source": [
        "def label2int(dataset):\n",
        "  y = []\n",
        "  for example in dataset:\n",
        "    if example[2].lower() == \"negative\":\n",
        "      y.append(0)\n",
        "    else:\n",
        "      y.append(1)\n",
        "  return y\n",
        "  \n",
        "y_train = label2int(train_original)\n",
        "y_dev = label2int(dev_original)\n",
        "y_test = label2int(test_original)\n",
        "y_train = np.array(y_train)\n",
        "y_dev = np.array(y_dev)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train[1])\n",
        "print(y_train[2])\n",
        "print(y_train[3])\n",
        "print(y_train[4])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txxIQPEdFSgY"
      },
      "source": [
        "We also generate these target labels using one hot encoding (This will be used in model 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohDm-E7k2w6c",
        "outputId": "e954c691-90fd-4e80-dbcd-a12b9a2fd6c7"
      },
      "source": [
        "def int2onehot(dataset):\n",
        "  y = []\n",
        "  for example in dataset:\n",
        "    if example:\n",
        "      y.append(np.array([0,1]))\n",
        "    else:\n",
        "      y.append(np.array([1,0]))\n",
        "  return np.array(y)\n",
        "y_train_onehot = int2onehot(y_train)\n",
        "y_dev_onehot = int2onehot(y_dev)\n",
        "y_test_onehot = int2onehot(y_test)\n",
        "\n",
        "print(y_train_onehot[0])\n",
        "print(y_train_onehot[1])\n",
        "print(y_train_onehot[2])\n",
        "print(y_train_onehot[3])\n",
        "print(y_train_onehot[4])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "Now we have almost done the data preprocessing. The easiest way is to input the tweet and topic as paired sentences into the model, concatenating them and separating them by the special BERT sentence-separator token [SEP] (available as tokenizer.sep_token). Then we can apply BERT directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKOiVVXQu-_I",
        "outputId": "ab9e9d65-b862-4f75-a81d-527d6bc3e3b8"
      },
      "source": [
        "# Please write your code to combine the tweet and topic into the following varibles\n",
        "# your code goes here\n",
        "#FOR TRAIN DATA (x_train_int, x_train_masks)\n",
        "x_train_int= []\n",
        "x_train_masks = []\n",
        "for train_data_concatenate in range(len(train_original)):\n",
        "  #Concatenating both train tweets and topics\n",
        "  train_join = str(train_original[train_data_concatenate][1]) + tokenizer.sep_token + str(train_original[train_data_concatenate][-1])\n",
        "  train_ids,train_masks,train_segments = tokenize(train_join,tokenizer)\n",
        "  #Append the train_int and train_masks to ids and masks\n",
        "  x_train_int.append(train_ids)\n",
        "  x_train_masks.append(train_masks)\n",
        "\n",
        "#FOR DEV DATA (x_dev_int, x_dev_masks)\n",
        "x_dev_int= []\n",
        "x_dev_masks = []\n",
        "for dev_data_concatenate in range(len(dev_original)):\n",
        "  #Concatenating both dev tweets and topics\n",
        "  dev_join = str(dev_original[dev_data_concatenate][1]) + tokenizer.sep_token + str(dev_original[dev_data_concatenate][-1])\n",
        "  dev_ids,dev_masks,dev_segments = tokenize(dev_join,tokenizer)\n",
        "  #Append the dev_int and dev_masks to ids and masks\n",
        "  x_dev_int.append(dev_ids)\n",
        "  x_dev_masks.append(dev_masks)\n",
        "\n",
        "#FOR TEST DATA (x_test_int, x_test_masks)\n",
        "x_test_int= []\n",
        "x_test_masks = []\n",
        "for test_data_concatenate in range(len(test_original)):\n",
        "  #Concatenating both test tweets and topics\n",
        "  test_join = str(test_original[test_data_concatenate][1]) + tokenizer.sep_token + str(test_original[test_data_concatenate][-1])\n",
        "  test_ids,test_masks,test_segments = tokenize(test_join,tokenizer)\n",
        "  #Append the test_int and test_masks to ids and masks\n",
        "  x_test_int.append(test_ids)\n",
        "  x_test_masks.append(test_masks)\n",
        "\n",
        "#Padding the train, dev, test integer and masked sequences\n",
        "X_train_int = keras.preprocessing.sequence.pad_sequences(x_train_int, padding='post', maxlen=128)\n",
        "X_dev_int = keras.preprocessing.sequence.pad_sequences(x_dev_int, padding='post', maxlen=128)\n",
        "X_test_int = keras.preprocessing.sequence.pad_sequences(x_test_int, padding='post', maxlen=128)\n",
        "X_train_masks = keras.preprocessing.sequence.pad_sequences(x_train_masks, padding='post', maxlen=128)\n",
        "X_dev_masks = keras.preprocessing.sequence.pad_sequences(x_dev_masks, padding='post', maxlen=128)\n",
        "X_train_masks = keras.preprocessing.sequence.pad_sequences(x_test_masks, padding='post', maxlen=128)\n",
        "\n",
        "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function\n",
        "#Wrapping the output of the pad sequences\n",
        "x_train_int_np = np.array(x_train_int)\n",
        "x_train_masks_np = np.array(x_train_masks)\n",
        "x_dev_int_np = np.array(x_dev_int)\n",
        "x_dev_masks_np = np.array(x_dev_masks)\n",
        "x_test_int_np = np.array(x_test_int)\n",
        "x_test_masks_np = np.array(x_test_masks)\n",
        "\n",
        "\n",
        "print(x_dev_int[0])\n",
        "print(x_dev_masks[0],'\\n')\n",
        "print(x_dev_int_np[0])\n",
        "print(x_dev_masks_np[0])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[  101  2745  4027   102  6108  1062  9794 16021 23091  2007 16839  9080\n",
            " 12863  7050  2000  2745  4027  1024  6108  1062  4593  2587 16021 23091\n",
            "  2006  5095  1998  1012  1012  8299  1024  1013  1013  1056  1012  2522\n",
            "  1013  1053  3501  2683  2072  2549  8586  2615 18037   102     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
            "\n",
            "[  101  2745  4027   102  6108  1062  9794 16021 23091  2007 16839  9080\n",
            " 12863  7050  2000  2745  4027  1024  6108  1062  4593  2587 16021 23091\n",
            "  2006  5095  1998  1012  1012  8299  1024  1013  1013  1056  1012  2522\n",
            "  1013  1053  3501  2683  2072  2549  8586  2615 18037   102     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgn7oTK4hTVz"
      },
      "source": [
        "In your report, show a few examples of your input, with text and topic included but separated by [SEP]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4enSQHhgMCH",
        "outputId": "b2a5f82b-1656-48f5-e01f-dd91f82fb6d8"
      },
      "source": [
        "#Examples of your inputs, with text and topic included but separated by [SEP]\n",
        "#FOR TRAIN DATA\n",
        "x_train= []\n",
        "for train_data_concatenate in range(len(train_original)):\n",
        "  #Concatenating both train tweets and topics using sep token\n",
        "  train_join = str(train_original[train_data_concatenate][1]) + tokenizer.sep_token + str(train_original[train_data_concatenate][-1])\n",
        "  train_inputs = tokenizer.tokenize(train_join)\n",
        "  #Append the train data to its sentences\n",
        "  x_train.append(train_inputs)\n",
        "\n",
        "#FOR DEV DATA\n",
        "x_dev= []\n",
        "for dev_data_concatenate in range(len(dev_original)):\n",
        "  #Concatenating both dev tweets and topics using sep token\n",
        "  dev_join = str(dev_original[dev_data_concatenate][1]) + tokenizer.sep_token + str(dev_original[dev_data_concatenate][-1])\n",
        "  dev_inputs = tokenizer.tokenize(dev_join)\n",
        "  #Append the dev data to its sentences\n",
        "  x_dev.append(dev_inputs)\n",
        "\n",
        "#FOR TEST DATA\n",
        "x_test= []\n",
        "for test_data_concatenate in range(len(test_original)):\n",
        "  #Concatenating both test tweets and topics using sep token\n",
        "  test_join = str(test_original[test_data_concatenate][1]) + tokenizer.sep_token + str(test_original[test_data_concatenate][-1])\n",
        "  test_inputs = tokenizer.tokenize(test_join)\n",
        "  #Append the test data to its sentences\n",
        "  x_test.append(test_inputs)\n",
        "\n",
        "print(x_train[0])\n",
        "print(x_dev[0])\n",
        "print(x_test[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'microsoft', '[SEP]', 'dear', '@', 'microsoft', 'the', 'new', '##oof', '##fi', '##ce', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'l', '##yn', '##c', 'update', '?', 'c', \"'\", 'mon', '.']\n",
            "['michael', 'jackson', '[SEP]', 'jay', 'z', 'joins', 'ins', '##tagram', 'with', 'nos', '##tal', '##gic', 'tribute', 'to', 'michael', 'jackson', ':', 'jay', 'z', 'apparently', 'joined', 'ins', '##tagram', 'on', 'saturday', 'and', '.', '.', 'http', ':', '/', '/', 't', '.', 'co', '/', 'q', '##j', '##9', '##i', '##4', '##ec', '##v', '##xy']\n",
            "['#', 'aria', '##na', '##gra', '##nde', '[SEP]', 'aria', '##na', 'grande', 'white', 'house', 'easter', 'egg', 'roll', 'in', 'washington', 'https', ':', '/', '/', 't', '.', 'co', '/', 'jd', '##j', '##l', '##9', '##sw', '##w', '##m', '##8', '#', 'aria', '##na', '##gra', '##nde']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqvUGIwwGJqu"
      },
      "source": [
        "#**Model 1: Prebuilt Sequence Classification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8f6ac1409a6f435f9b4a24de9543d07c",
            "4ddb99db850e4c17a97f567e114ce094",
            "e64b21e847a44699860c4acbc9a2d990",
            "60db75f05d144115bb61f0abd66423d8",
            "a8102393adba4f4a87da4ad365d05068",
            "86b104c09c4646358f510c2b8e017d4f",
            "352b71afb13c425e9a2cde4f81bdbaa2",
            "4e7c0c21d5e547d4b40bfbd2f22098b5"
          ]
        },
        "id": "1gXFbb2cxBlw",
        "outputId": "5c1d9c9a-a155-4eaf-fd1e-f80121c779cc"
      },
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
        "import tensorflow as tf\n",
        "\n",
        "distil_bert = 'distilbert-base-uncased'\n",
        "\n",
        "config = DistilBertConfig(num_labels=2)\n",
        "config.output_hidden_states = False\n",
        "\n",
        "def create_TFDistilBertForSequenceClassification():\n",
        "  transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)\n",
        "  input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
        "  X = transformer_model(input_ids, input_masks_ids)\n",
        "  return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model on TPU:\n",
        "  with strategy.scope():\n",
        "    model = create_TFDistilBertForSequenceClassification()\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model = create_TFDistilBertForSequenceClassification()\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.102.219.234:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.102.219.234:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f6ac1409a6f435f9b4a24de9543d07c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc65b2cbd70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc65b2cbd70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc65b2cbd70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc676b75c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc676b75c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fc676b75c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CPFj0CMx9mw",
        "outputId": "a5bc0384-cadf-4f76-e764-764bc2136582"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_for_sequence_cla TFSequenceClassifier 66955010    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 66,955,010\n",
            "Trainable params: 66,955,010\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQQH5lE_33Vn",
        "outputId": "31ecbcac-ad39-4543-eeaa-3393f64ad988"
      },
      "source": [
        "history = model.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train_onehot,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev_onehot),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.5321WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 90s 1s/step - loss: 0.8958 - accuracy: 0.5344 - val_loss: 0.5275 - val_accuracy: 0.7442\n",
            "Epoch 2/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.4684 - accuracy: 0.7886 - val_loss: 0.3995 - val_accuracy: 0.7577\n",
            "Epoch 3/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.3574 - accuracy: 0.8434 - val_loss: 0.3527 - val_accuracy: 0.8521\n",
            "Epoch 4/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.2990 - accuracy: 0.8929 - val_loss: 0.3822 - val_accuracy: 0.8551\n",
            "Epoch 5/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.2584 - accuracy: 0.9113 - val_loss: 0.3630 - val_accuracy: 0.8415\n",
            "Epoch 6/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.2240 - accuracy: 0.9203 - val_loss: 0.4778 - val_accuracy: 0.8513\n",
            "Epoch 7/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.1908 - accuracy: 0.9431 - val_loss: 0.4655 - val_accuracy: 0.8468\n",
            "Epoch 8/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.1527 - accuracy: 0.9545 - val_loss: 0.6908 - val_accuracy: 0.8566\n",
            "Epoch 9/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.1157 - accuracy: 0.9684 - val_loss: 0.7513 - val_accuracy: 0.8385\n",
            "Epoch 10/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.1106 - accuracy: 0.9728 - val_loss: 0.5094 - val_accuracy: 0.8400\n",
            "Epoch 11/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.1114 - accuracy: 0.9712 - val_loss: 0.8860 - val_accuracy: 0.8498\n",
            "Epoch 12/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0963 - accuracy: 0.9786 - val_loss: 0.7462 - val_accuracy: 0.8287\n",
            "Epoch 13/30\n",
            "35/35 [==============================] - 7s 208ms/step - loss: 0.1397 - accuracy: 0.9623 - val_loss: 0.8507 - val_accuracy: 0.8468\n",
            "Epoch 14/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0641 - accuracy: 0.9869 - val_loss: 1.2433 - val_accuracy: 0.8392\n",
            "Epoch 15/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.0605 - accuracy: 0.9898 - val_loss: 1.1940 - val_accuracy: 0.8445\n",
            "Epoch 16/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0436 - accuracy: 0.9939 - val_loss: 1.4764 - val_accuracy: 0.8483\n",
            "Epoch 17/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0364 - accuracy: 0.9962 - val_loss: 1.5562 - val_accuracy: 0.8498\n",
            "Epoch 18/30\n",
            "35/35 [==============================] - 8s 219ms/step - loss: 0.0310 - accuracy: 0.9973 - val_loss: 1.5879 - val_accuracy: 0.8513\n",
            "Epoch 19/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0274 - accuracy: 0.9977 - val_loss: 1.6630 - val_accuracy: 0.8536\n",
            "Epoch 20/30\n",
            "35/35 [==============================] - 7s 208ms/step - loss: 0.0295 - accuracy: 0.9970 - val_loss: 1.6338 - val_accuracy: 0.8468\n",
            "Epoch 21/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0656 - accuracy: 0.9910 - val_loss: 1.4550 - val_accuracy: 0.8204\n",
            "Epoch 22/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0830 - accuracy: 0.9820 - val_loss: 1.5017 - val_accuracy: 0.8196\n",
            "Epoch 23/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.4099 - accuracy: 0.8207 - val_loss: 0.5604 - val_accuracy: 0.7442\n",
            "Epoch 24/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.4563 - accuracy: 0.7920 - val_loss: 0.4902 - val_accuracy: 0.7992\n",
            "Epoch 25/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.3350 - accuracy: 0.8880 - val_loss: 0.5864 - val_accuracy: 0.8128\n",
            "Epoch 26/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.2314 - accuracy: 0.9259 - val_loss: 0.6576 - val_accuracy: 0.7849\n",
            "Epoch 27/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.1798 - accuracy: 0.9504 - val_loss: 1.0672 - val_accuracy: 0.8264\n",
            "Epoch 28/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.1093 - accuracy: 0.9715 - val_loss: 0.9028 - val_accuracy: 0.8317\n",
            "Epoch 29/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0898 - accuracy: 0.9803 - val_loss: 1.0053 - val_accuracy: 0.8340\n",
            "Epoch 30/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0809 - accuracy: 0.9836 - val_loss: 1.1717 - val_accuracy: 0.8498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQcytuBdaWVp",
        "outputId": "a6c840f4-429d-46fc-abd8-b9aaaa569e5f"
      },
      "source": [
        "results = model.evaluate([x_test_int_np,x_test_masks_np], y_test_onehot)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194/194 [==============================] - 9s 31ms/step - loss: 1.3371 - accuracy: 0.8372\n",
            "[1.3371375799179077, 0.8371867537498474]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX9FS0RauHvG"
      },
      "source": [
        "##**INFERENCE:**\n",
        "##**Comparison of Lab5 with model1 (Prebuilt Sequence Classification model)**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words without pre-trained word embeddings model)* -- **0.7528**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words with pre-trained word embeddings model)* -- **0.7876**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words model with multiple-input)* -- **0.7738**\n",
        "\n",
        "*Accuracy with model1(Prebuilt Sequence Classification)* -- **0.8372**\n",
        "\n",
        "##**Reason:**\n",
        "* The model1 accuracy which is the Prebuilt Sequence Classification model acheives **high accuracy** than models which was created in lab5. This is because BERT is bidirectional which combines Masks with the sentence prediction. it is used in predicting the missing words in the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZ4nl08vp9A"
      },
      "source": [
        "\n",
        "#**Model 2: Neural bag of words using BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStlnRQRf-4v"
      },
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fTwmYDvNEyT"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n",
        "def get_BERT_layer():\n",
        "  distil_bert = 'distilbert-base-uncased'\n",
        "  config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "  config.output_hidden_states = False\n",
        "  return TFDistilBertModel.from_pretrained(distil_bert, config = config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VICS9rY8C7KH",
        "outputId": "82f82d64-2015-4f21-f93b-5480752a1c22"
      },
      "source": [
        "hdepth=16\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "EMBED_SIZE=100\n",
        "\n",
        "\n",
        "def create_bag_of_words_BERT():\n",
        "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "  bert_embeddings = get_BERT_layer()\n",
        "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "\n",
        "  pooled_sent=GlobalAveragePooling1DMasked()(embedded_sent)\n",
        "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(pooled_sent) # Sigmoid\n",
        "  label=Dense(1,input_shape=(hdepth,),activation='sigmoid',kernel_initializer='glorot_uniform')(hidden_output)\n",
        "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model\n",
        "  with strategy.scope():\n",
        "    model2 = create_bag_of_words_BERT()\n",
        "    optimizer2 = keras.optimizers.Adam(lr=5e-5)\n",
        "    model2.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model2 = create_bag_of_words_BERT()\n",
        "  model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model2.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"Model2_BERT\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_masked (None, 768)          0           tf_distil_bert_model[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           12304       global_average_pooling1d_masked[0\n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            17          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 66,375,201\n",
            "Trainable params: 66,375,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0SbsCsxF1zi",
        "outputId": "15b0f0c1-800e-4621-9b62-5eb0b67e682e"
      },
      "source": [
        "history = model2.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8169WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 92s 1s/step - loss: 0.4319 - accuracy: 0.8177 - val_loss: 0.3266 - val_accuracy: 0.8619\n",
            "Epoch 2/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.2545 - accuracy: 0.9027 - val_loss: 0.3213 - val_accuracy: 0.8528\n",
            "Epoch 3/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.2012 - accuracy: 0.9329 - val_loss: 0.3229 - val_accuracy: 0.8513\n",
            "Epoch 4/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.1667 - accuracy: 0.9524 - val_loss: 0.3535 - val_accuracy: 0.8430\n",
            "Epoch 5/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.1437 - accuracy: 0.9646 - val_loss: 0.3400 - val_accuracy: 0.8558\n",
            "Epoch 6/30\n",
            "35/35 [==============================] - 7s 203ms/step - loss: 0.1286 - accuracy: 0.9722 - val_loss: 0.3692 - val_accuracy: 0.8438\n",
            "Epoch 7/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.1188 - accuracy: 0.9764 - val_loss: 0.3441 - val_accuracy: 0.8528\n",
            "Epoch 8/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.1135 - accuracy: 0.9794 - val_loss: 0.3450 - val_accuracy: 0.8574\n",
            "Epoch 9/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.1101 - accuracy: 0.9808 - val_loss: 0.3737 - val_accuracy: 0.8460\n",
            "Epoch 10/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.1075 - accuracy: 0.9818 - val_loss: 0.3816 - val_accuracy: 0.8445\n",
            "Epoch 11/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.1048 - accuracy: 0.9847 - val_loss: 0.4029 - val_accuracy: 0.8581\n",
            "Epoch 12/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0977 - accuracy: 0.9872 - val_loss: 0.3959 - val_accuracy: 0.8453\n",
            "Epoch 13/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.0958 - accuracy: 0.9872 - val_loss: 0.3867 - val_accuracy: 0.8491\n",
            "Epoch 14/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0941 - accuracy: 0.9873 - val_loss: 0.3881 - val_accuracy: 0.8581\n",
            "Epoch 15/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0904 - accuracy: 0.9886 - val_loss: 0.4019 - val_accuracy: 0.8498\n",
            "Epoch 16/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0918 - accuracy: 0.9881 - val_loss: 0.3992 - val_accuracy: 0.8528\n",
            "Epoch 17/30\n",
            "35/35 [==============================] - 7s 208ms/step - loss: 0.0904 - accuracy: 0.9880 - val_loss: 0.4148 - val_accuracy: 0.8483\n",
            "Epoch 18/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.0857 - accuracy: 0.9908 - val_loss: 0.4135 - val_accuracy: 0.8415\n",
            "Epoch 19/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0825 - accuracy: 0.9916 - val_loss: 0.4017 - val_accuracy: 0.8400\n",
            "Epoch 20/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0813 - accuracy: 0.9913 - val_loss: 0.3882 - val_accuracy: 0.8513\n",
            "Epoch 21/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0851 - accuracy: 0.9904 - val_loss: 0.4095 - val_accuracy: 0.8423\n",
            "Epoch 22/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0803 - accuracy: 0.9915 - val_loss: 0.4301 - val_accuracy: 0.8362\n",
            "Epoch 23/30\n",
            "35/35 [==============================] - 7s 205ms/step - loss: 0.0823 - accuracy: 0.9908 - val_loss: 0.4340 - val_accuracy: 0.8445\n",
            "Epoch 24/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0801 - accuracy: 0.9918 - val_loss: 0.4273 - val_accuracy: 0.8355\n",
            "Epoch 25/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0760 - accuracy: 0.9923 - val_loss: 0.4354 - val_accuracy: 0.8475\n",
            "Epoch 26/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0756 - accuracy: 0.9922 - val_loss: 0.4349 - val_accuracy: 0.8392\n",
            "Epoch 27/30\n",
            "35/35 [==============================] - 7s 204ms/step - loss: 0.0755 - accuracy: 0.9923 - val_loss: 0.4205 - val_accuracy: 0.8408\n",
            "Epoch 28/30\n",
            "35/35 [==============================] - 7s 206ms/step - loss: 0.0750 - accuracy: 0.9924 - val_loss: 0.4410 - val_accuracy: 0.8340\n",
            "Epoch 29/30\n",
            "35/35 [==============================] - 8s 228ms/step - loss: 0.0738 - accuracy: 0.9930 - val_loss: 0.4317 - val_accuracy: 0.8392\n",
            "Epoch 30/30\n",
            "35/35 [==============================] - 7s 207ms/step - loss: 0.0727 - accuracy: 0.9931 - val_loss: 0.4429 - val_accuracy: 0.8294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0_vvG6UQtv",
        "outputId": "51b447c8-e819-43ab-98be-2eb1a5372dd5"
      },
      "source": [
        "results = model2.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194/194 [==============================] - 8s 31ms/step - loss: 0.4170 - accuracy: 0.8863\n",
            "[0.41697075963020325, 0.8863378763198853]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUFsZ2DPrU_-"
      },
      "source": [
        "##**INFERENCE:**\n",
        "##**Comparison of model1 and Lab5 with model2 (Neural bag of words using BERT model)**\n",
        "\n",
        "*Accuracy with model1(Prebuilt Sequence Classification)* -- **0.8372**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words without pre-trained word embeddings model)* -- **0.7528**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words with pre-trained word embeddings model)* -- **0.7876**\n",
        "\n",
        "*Accuracy of the Lab5 (Neural bag of words model with multiple-input)* -- **0.7738**\n",
        "\n",
        "*Accuracy with model2 (Neural bag of words using BERT)*  -- **0.8863**\n",
        "\n",
        "##**Reason:**\n",
        "* The accuracy of model2 which is the Neural bag of words using BERT model acheives **high accuracy** than Prebuilt Sequence Classification model and the model which was developed in lab5. This model produces high accuracy because it is bidirectional which combines Masks with the sentence prediction. it is used in predicting the missing words in the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awOphcCnhEwv"
      },
      "source": [
        "#**Model 3: CNN or LSTM with BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yUt7XgTqpmA"
      },
      "source": [
        "#**LSTM model with BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMiiWhW4hPRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11ed4ca-7860-4243-eb3f-7a0083b7130b"
      },
      "source": [
        "# your code goes here\n",
        "hdepth=16\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "EMBED_SIZE=100\n",
        "\n",
        "def create_bag_of_words_BERT_LSTM():\n",
        "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "  bert_embeddings = get_BERT_layer()\n",
        "  #Embedding layer\n",
        "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "  #LSTM layer\n",
        "  lstm_layer  = LSTM(150, return_sequences=False)(embedded_sent)\n",
        "  #Hidden layer\n",
        "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(lstm_layer)\n",
        "  #Output layer \n",
        "  label=Dense(1,input_shape=(hdepth,),activation='sigmoid',kernel_initializer='glorot_uniform')(hidden_output)\n",
        "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model\n",
        "  with strategy.scope():\n",
        "    model3a = create_bag_of_words_BERT_LSTM()\n",
        "    optimizer2 = keras.optimizers.Adam(lr=5e-5)\n",
        "    model3a.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model3a = create_bag_of_words_BERT_LSTM()\n",
        "  model3a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model3a.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"Model2_BERT\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 150)          551400      tf_distil_bert_model_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 16)           2416        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            17          dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 66,916,713\n",
            "Trainable params: 66,916,713\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSFzSIdrbAj",
        "outputId": "be391436-a9da-49d5-9cd8-d4acef8aa38c"
      },
      "source": [
        "#training the model\n",
        "history = model3a.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.6251WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r35/35 [==============================] - 95s 1s/step - loss: 0.6432 - accuracy: 0.6283 - val_loss: 0.5715 - val_accuracy: 0.7442\n",
            "Epoch 2/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.5228 - accuracy: 0.7969 - val_loss: 0.5244 - val_accuracy: 0.7442\n",
            "Epoch 3/30\n",
            "35/35 [==============================] - 8s 215ms/step - loss: 0.4747 - accuracy: 0.8382 - val_loss: 0.4961 - val_accuracy: 0.7985\n",
            "Epoch 4/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.4090 - accuracy: 0.8905 - val_loss: 0.4337 - val_accuracy: 0.8551\n",
            "Epoch 5/30\n",
            "35/35 [==============================] - 8s 218ms/step - loss: 0.3574 - accuracy: 0.9303 - val_loss: 0.4373 - val_accuracy: 0.8626\n",
            "Epoch 6/30\n",
            "35/35 [==============================] - 8s 219ms/step - loss: 0.3283 - accuracy: 0.9486 - val_loss: 0.4262 - val_accuracy: 0.8642\n",
            "Epoch 7/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.3151 - accuracy: 0.9550 - val_loss: 0.4297 - val_accuracy: 0.8657\n",
            "Epoch 8/30\n",
            "35/35 [==============================] - 7s 215ms/step - loss: 0.3071 - accuracy: 0.9582 - val_loss: 0.4297 - val_accuracy: 0.8672\n",
            "Epoch 9/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2945 - accuracy: 0.9634 - val_loss: 0.4500 - val_accuracy: 0.8574\n",
            "Epoch 10/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2875 - accuracy: 0.9646 - val_loss: 0.4301 - val_accuracy: 0.8649\n",
            "Epoch 11/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2775 - accuracy: 0.9711 - val_loss: 0.4580 - val_accuracy: 0.8475\n",
            "Epoch 12/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2761 - accuracy: 0.9697 - val_loss: 0.4476 - val_accuracy: 0.8513\n",
            "Epoch 13/30\n",
            "35/35 [==============================] - 7s 215ms/step - loss: 0.2666 - accuracy: 0.9748 - val_loss: 0.4391 - val_accuracy: 0.8574\n",
            "Epoch 14/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2593 - accuracy: 0.9775 - val_loss: 0.4588 - val_accuracy: 0.8475\n",
            "Epoch 15/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2591 - accuracy: 0.9755 - val_loss: 0.4232 - val_accuracy: 0.8657\n",
            "Epoch 16/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2588 - accuracy: 0.9744 - val_loss: 0.4475 - val_accuracy: 0.8558\n",
            "Epoch 17/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2502 - accuracy: 0.9778 - val_loss: 0.4637 - val_accuracy: 0.8453\n",
            "Epoch 18/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2491 - accuracy: 0.9768 - val_loss: 0.4673 - val_accuracy: 0.8460\n",
            "Epoch 19/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2466 - accuracy: 0.9781 - val_loss: 0.4565 - val_accuracy: 0.8521\n",
            "Epoch 20/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2430 - accuracy: 0.9780 - val_loss: 0.4659 - val_accuracy: 0.8468\n",
            "Epoch 21/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2422 - accuracy: 0.9775 - val_loss: 0.4402 - val_accuracy: 0.8566\n",
            "Epoch 22/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2375 - accuracy: 0.9788 - val_loss: 0.4468 - val_accuracy: 0.8483\n",
            "Epoch 23/30\n",
            "35/35 [==============================] - 7s 215ms/step - loss: 0.2335 - accuracy: 0.9807 - val_loss: 0.4645 - val_accuracy: 0.8491\n",
            "Epoch 24/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2335 - accuracy: 0.9792 - val_loss: 0.4322 - val_accuracy: 0.8475\n",
            "Epoch 25/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2333 - accuracy: 0.9769 - val_loss: 0.4862 - val_accuracy: 0.8400\n",
            "Epoch 26/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2301 - accuracy: 0.9787 - val_loss: 0.4831 - val_accuracy: 0.8347\n",
            "Epoch 27/30\n",
            "35/35 [==============================] - 8s 216ms/step - loss: 0.2251 - accuracy: 0.9806 - val_loss: 0.4607 - val_accuracy: 0.8453\n",
            "Epoch 28/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2216 - accuracy: 0.9815 - val_loss: 0.4477 - val_accuracy: 0.8543\n",
            "Epoch 29/30\n",
            "35/35 [==============================] - 8s 217ms/step - loss: 0.2186 - accuracy: 0.9826 - val_loss: 0.4448 - val_accuracy: 0.8491\n",
            "Epoch 30/30\n",
            "35/35 [==============================] - 9s 247ms/step - loss: 0.2180 - accuracy: 0.9816 - val_loss: 0.4735 - val_accuracy: 0.8370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i9dto8drbI9",
        "outputId": "f668c71c-ae79-44f9-95d4-66eadc233b6a"
      },
      "source": [
        "#Evaluating on test data\n",
        "results = model3a.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194/194 [==============================] - 9s 34ms/step - loss: 0.3340 - accuracy: 0.8888\n",
            "[0.3340151906013489, 0.8887631297111511]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szgllyMesAON"
      },
      "source": [
        "#**CNN model with BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD6rCk-CsEUF",
        "outputId": "217a5e01-1d4b-425b-a1c7-e0f2d6316b6e"
      },
      "source": [
        "hdepth=16\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "EMBED_SIZE=100\n",
        "\n",
        "def create_bag_of_words_BERT_CNN():\n",
        "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "  bert_embeddings = get_BERT_layer()\n",
        "  #Embedding layer\n",
        "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "  #CNN layer\n",
        "  cnn_layer  = Conv1D(512, kernel_size = 8, activation=\"relu\", strides=3)(embedded_sent)\n",
        "  #Hidden layer\n",
        "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(cnn_layer)\n",
        "  #Output layer \n",
        "  label=Dense(1,input_shape=(hdepth,),activation='sigmoid',kernel_initializer='glorot_uniform')(hidden_output)\n",
        "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model\n",
        "  with strategy.scope():\n",
        "    model3b = create_bag_of_words_BERT_CNN()\n",
        "    optimizer2 = keras.optimizers.Adam(lr=5e-5)\n",
        "    model3b.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model3b = create_bag_of_words_BERT_CNN()\n",
        "  model3b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model3b.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.37.91.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.91.122:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"Model2_BERT\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_2 (TFDisti TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 41, 512)      3146240     tf_distil_bert_model_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 41, 16)       8208        conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 41, 1)        17          dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 69,517,345\n",
            "Trainable params: 69,517,345\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1QYO7bRsEdz",
        "outputId": "31f1d92f-12df-4ca2-f5bd-d569a71d052a"
      },
      "source": [
        "#training the model\n",
        "history = model3b.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8170WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 97s 1s/step - loss: 0.4279 - accuracy: 0.8179 - val_loss: 0.3177 - val_accuracy: 0.8658\n",
            "Epoch 2/30\n",
            "35/35 [==============================] - 7s 210ms/step - loss: 0.2262 - accuracy: 0.9094 - val_loss: 0.3389 - val_accuracy: 0.8344\n",
            "Epoch 3/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.1804 - accuracy: 0.9328 - val_loss: 0.3175 - val_accuracy: 0.8562\n",
            "Epoch 4/30\n",
            "35/35 [==============================] - 7s 210ms/step - loss: 0.1401 - accuracy: 0.9535 - val_loss: 0.3623 - val_accuracy: 0.8459\n",
            "Epoch 5/30\n",
            "35/35 [==============================] - 7s 209ms/step - loss: 0.1230 - accuracy: 0.9614 - val_loss: 0.4074 - val_accuracy: 0.8349\n",
            "Epoch 6/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0944 - accuracy: 0.9769 - val_loss: 0.4143 - val_accuracy: 0.8366\n",
            "Epoch 7/30\n",
            "35/35 [==============================] - 7s 210ms/step - loss: 0.0760 - accuracy: 0.9830 - val_loss: 0.4129 - val_accuracy: 0.8404\n",
            "Epoch 8/30\n",
            "35/35 [==============================] - 7s 213ms/step - loss: 0.0776 - accuracy: 0.9815 - val_loss: 0.4087 - val_accuracy: 0.8525\n",
            "Epoch 9/30\n",
            "35/35 [==============================] - 7s 209ms/step - loss: 0.0659 - accuracy: 0.9866 - val_loss: 0.4313 - val_accuracy: 0.8375\n",
            "Epoch 10/30\n",
            "35/35 [==============================] - 7s 210ms/step - loss: 0.0616 - accuracy: 0.9890 - val_loss: 0.4265 - val_accuracy: 0.8445\n",
            "Epoch 11/30\n",
            "35/35 [==============================] - 7s 210ms/step - loss: 0.0609 - accuracy: 0.9895 - val_loss: 0.4318 - val_accuracy: 0.8562\n",
            "Epoch 12/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0533 - accuracy: 0.9924 - val_loss: 0.4738 - val_accuracy: 0.8468\n",
            "Epoch 13/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0557 - accuracy: 0.9913 - val_loss: 0.4455 - val_accuracy: 0.8499\n",
            "Epoch 14/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0515 - accuracy: 0.9929 - val_loss: 0.4794 - val_accuracy: 0.8174\n",
            "Epoch 15/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0610 - accuracy: 0.9881 - val_loss: 0.4563 - val_accuracy: 0.8421\n",
            "Epoch 16/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0484 - accuracy: 0.9942 - val_loss: 0.4644 - val_accuracy: 0.8520\n",
            "Epoch 17/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0526 - accuracy: 0.9920 - val_loss: 0.4609 - val_accuracy: 0.8469\n",
            "Epoch 18/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0484 - accuracy: 0.9931 - val_loss: 0.4549 - val_accuracy: 0.8488\n",
            "Epoch 19/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0461 - accuracy: 0.9943 - val_loss: 0.4804 - val_accuracy: 0.8297\n",
            "Epoch 20/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0430 - accuracy: 0.9955 - val_loss: 0.4859 - val_accuracy: 0.8511\n",
            "Epoch 21/30\n",
            "35/35 [==============================] - 7s 213ms/step - loss: 0.0443 - accuracy: 0.9951 - val_loss: 0.5167 - val_accuracy: 0.8483\n",
            "Epoch 22/30\n",
            "35/35 [==============================] - 7s 213ms/step - loss: 0.0470 - accuracy: 0.9937 - val_loss: 0.4685 - val_accuracy: 0.8480\n",
            "Epoch 23/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0438 - accuracy: 0.9951 - val_loss: 0.4835 - val_accuracy: 0.8463\n",
            "Epoch 24/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0435 - accuracy: 0.9944 - val_loss: 0.4630 - val_accuracy: 0.8413\n",
            "Epoch 25/30\n",
            "35/35 [==============================] - 7s 214ms/step - loss: 0.0414 - accuracy: 0.9956 - val_loss: 0.4774 - val_accuracy: 0.8467\n",
            "Epoch 26/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0406 - accuracy: 0.9957 - val_loss: 0.5067 - val_accuracy: 0.8412\n",
            "Epoch 27/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0405 - accuracy: 0.9957 - val_loss: 0.5003 - val_accuracy: 0.8416\n",
            "Epoch 28/30\n",
            "35/35 [==============================] - 7s 212ms/step - loss: 0.0416 - accuracy: 0.9950 - val_loss: 0.4984 - val_accuracy: 0.8320\n",
            "Epoch 29/30\n",
            "35/35 [==============================] - 7s 211ms/step - loss: 0.0383 - accuracy: 0.9964 - val_loss: 0.5050 - val_accuracy: 0.8494\n",
            "Epoch 30/30\n",
            "35/35 [==============================] - 7s 213ms/step - loss: 0.0392 - accuracy: 0.9954 - val_loss: 0.4788 - val_accuracy: 0.8352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e77pQtwksEo3",
        "outputId": "ea9c1ecd-f914-4515-80df-d8dcf04c5389"
      },
      "source": [
        "#Evaluating on test data\n",
        "results = model3b.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194/194 [==============================] - 9s 33ms/step - loss: 0.3719 - accuracy: 0.8918\n",
            "[0.3718743920326233, 0.8917599320411682]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8PDTdTbTUg"
      },
      "source": [
        "#**INFERENCE:**\n",
        "##**Comparison of model1 and model2 with model3(LSTM model):**\n",
        "\n",
        "*Accuracy with model1(Prebuilt Sequence Classification)* -- **0.8372**\n",
        "\n",
        "*Accuracy with model2 (Neural bag of words using BERT)*  -- **0.8863**\n",
        "\n",
        "*Accuracy with model3 (LSTM model with BERT)*           -- **0.8888**\n",
        "##**Reason:**\n",
        "* The accuracy of model3 which is the LSTM model with BERT acheives high accuracy than Prebuilt Sequence Classification model and the Neural bags of words model.\n",
        "* LSTM model acheives high accuracy because it is assigned with the LSTM units so that the backpropagation is performed better. although there is very minimal increase in the accuracy between model2 and model3. \n",
        "\n",
        "##**Comparison of model1 and model2 with model3(CNN model):**\n",
        "\n",
        "*Accuracy with model1(Prebuilt Sequence Classification)* -- **0.8372**\n",
        "\n",
        "*Accuracy with model2 (Neural bag of words using BERT)*  -- **0.8863**\n",
        "\n",
        "*Accuracy with model3 (LSTM model with BERT)*           -- **0.8918**\n",
        "##**Reason:**\n",
        "* The model3 which is the CNN model with BERT acheives high accuracy than Prebuilt Sequence Classification model(model1) and the Neural bags of words model(model2).\n",
        "* CNN model acheives high accuracy because the data is parsed through the convolutional layers and so that the backpropagation is performed better and the loss will be decreased gradually. \n"
      ]
    }
  ]
}